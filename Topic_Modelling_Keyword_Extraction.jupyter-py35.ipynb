{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Load and install necessary packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# !pip install --user gensim \n",
    "# !pip install --user gensim pyLDAvis wordlcoud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Read the Dataset into a dataframe with apt encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ConcernID', 'IssueID', 'ConcernStatus', 'Team', 'ReceivedDate',\n",
       "       'AcknowledgementDate', 'ResolutionDate', 'ANZReceivedDate',\n",
       "       'HardshipDate', '21DayLetterDueDate', '45DayLetterDueDate',\n",
       "       'ResponseDueDate', 'AcknowledgementTurnAroundTime',\n",
       "       'ResolutionTurnAroundTime', 'IssueType', 'Channel', 'Breach',\n",
       "       'ConcernDescription', 'IssueDescription', 'ResolutionType',\n",
       "       'ResolutionDescription', 'EncounterCategory', 'EncounterType',\n",
       "       'BusinessArea', 'BusinessDepartment', 'ProductCategory', 'ProductType',\n",
       "       'FOSFlag', 'FOSFileNumber', 'ChargedToBU', 'Tribunal', 'Segment',\n",
       "       'Clause'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, pandas as pd\n",
    "# Add asset from file system\n",
    "complaints_df = pd.read_csv(os.environ['DSX_PROJECT_DIR']+'/datasets/Complaints_All_2018_Consolidated_Final_For_upload.csv',\n",
    "                            encoding='ISO-8859-1')\n",
    "complaints_df.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ConcernID</th>\n",
       "      <th>IssueID</th>\n",
       "      <th>ConcernStatus</th>\n",
       "      <th>Team</th>\n",
       "      <th>ReceivedDate</th>\n",
       "      <th>AcknowledgementDate</th>\n",
       "      <th>ResolutionDate</th>\n",
       "      <th>ANZReceivedDate</th>\n",
       "      <th>HardshipDate</th>\n",
       "      <th>21DayLetterDueDate</th>\n",
       "      <th>...</th>\n",
       "      <th>BusinessArea</th>\n",
       "      <th>BusinessDepartment</th>\n",
       "      <th>ProductCategory</th>\n",
       "      <th>ProductType</th>\n",
       "      <th>FOSFlag</th>\n",
       "      <th>FOSFileNumber</th>\n",
       "      <th>ChargedToBU</th>\n",
       "      <th>Tribunal</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Clause</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>964163</td>\n",
       "      <td>1016404</td>\n",
       "      <td>CLOSE</td>\n",
       "      <td>Internal Dispute Team</td>\n",
       "      <td>2017-07-18</td>\n",
       "      <td>2017-05-24</td>\n",
       "      <td>2017-09-05</td>\n",
       "      <td>2017-05-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-06-13</td>\n",
       "      <td>...</td>\n",
       "      <td>Operations</td>\n",
       "      <td>Other Servicing</td>\n",
       "      <td>Not Product related</td>\n",
       "      <td>Not Product related</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>965539</td>\n",
       "      <td>1017305</td>\n",
       "      <td>CLOSE</td>\n",
       "      <td>Internal Dispute Team</td>\n",
       "      <td>2017-07-19</td>\n",
       "      <td>2017-06-02</td>\n",
       "      <td>2017-08-24</td>\n",
       "      <td>2017-06-02</td>\n",
       "      <td>2016-12-06</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>...</td>\n",
       "      <td>Operations</td>\n",
       "      <td>Collections / Hardship</td>\n",
       "      <td>Business Lending</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Small Business Banking</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>967004</td>\n",
       "      <td>1015985</td>\n",
       "      <td>CLOSE</td>\n",
       "      <td>Internal Dispute Team</td>\n",
       "      <td>2017-07-02</td>\n",
       "      <td>2017-06-14</td>\n",
       "      <td>2017-08-23</td>\n",
       "      <td>2017-06-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-07-04</td>\n",
       "      <td>...</td>\n",
       "      <td>Products</td>\n",
       "      <td>Mortgages</td>\n",
       "      <td>Mortgages</td>\n",
       "      <td>Variable Rate Home Loan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ConcernID  IssueID ConcernStatus                   Team ReceivedDate  \\\n",
       "0    964163  1016404         CLOSE  Internal Dispute Team   2017-07-18   \n",
       "1    965539  1017305         CLOSE  Internal Dispute Team   2017-07-19   \n",
       "2    967004  1015985         CLOSE  Internal Dispute Team   2017-07-02   \n",
       "\n",
       "  AcknowledgementDate ResolutionDate ANZReceivedDate HardshipDate  \\\n",
       "0          2017-05-24     2017-09-05      2017-05-24          NaN   \n",
       "1          2017-06-02     2017-08-24      2017-06-02   2016-12-06   \n",
       "2          2017-06-14     2017-08-23      2017-06-14          NaN   \n",
       "\n",
       "  21DayLetterDueDate  ...   BusinessArea      BusinessDepartment  \\\n",
       "0         2017-06-13  ...     Operations         Other Servicing   \n",
       "1         2017-06-22  ...     Operations  Collections / Hardship   \n",
       "2         2017-07-04  ...       Products               Mortgages   \n",
       "\n",
       "       ProductCategory              ProductType FOSFlag FOSFileNumber  \\\n",
       "0  Not Product related      Not Product related     NaN           NaN   \n",
       "1     Business Lending                   Other      NaN           NaN   \n",
       "2            Mortgages  Variable Rate Home Loan     NaN           NaN   \n",
       "\n",
       "  ChargedToBU Tribunal                 Segment Clause  \n",
       "0         NaN      NaN                     NaN     13  \n",
       "1         NaN      NaN  Small Business Banking     32  \n",
       "2         NaN      NaN                     NaN     13  \n",
       "\n",
       "[3 rows x 33 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complaints_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Combine the text corpus to create the final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Unclaimed Monies  - ANZ Account - $1,978,74. I...\n",
       "1    Customer is unable to pay his business loans s...\n",
       "2    I am writing to express our grave concern abou...\n",
       "Name: keyword_corpus, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns= ['ConcernDescription','IssueDescription','ResolutionDescription']\n",
    "keywords_df = complaints_df[columns]\n",
    "keywords_df['keyword_corpus'] = keywords_df[columns[0]]+\".\"+keywords_df[columns[1]]+\".\"+keywords_df[columns[2]]\n",
    "keywords_df['keyword_corpus'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Unclaimed Monies  - ANZ Account - $1,978,74. I am emailing from London as a former Sydney ANZ customer and have been trying to claim my ANZ bank account money ($1,978.74,) for over 12 months now. Over 12 months ago submitted by post all of the required documentation, including the ANZ unclaimed monies form plus all of the certified Proof of ID documentation and bank account details, together with an ASICs print out showing that $1,978.74 is listed as unclaimed funds under my name and current London address (See Below.) Once again this year I submitted a request by post from London to 'ANZ Unclaimed Monies Melbourne' for the above amount in February 2017 and it was signed for in Melbourne on 7 March 2017 at the below address. Once again I included all of the relevant applications forms, bank details and other required certified supporting documentation. You should have everything you require to complete the transfer..unclaimed money request.CA - 5 September 2017 - Feedback noted. Estates confirm funds remitted to the customer on 22 August 2017.22 June 2017 - Referred to unclaimed monies and deceased estates team. Unclaimed Monies team have responsed to cust on 22 june 2017. All necessary d\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_df['keyword_corpus'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Clean the data with regex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#remove email\n",
    "def noemail(string):\n",
    "    string = re.sub(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+',' ',string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#clean the strings\n",
    "def cleanstring(string):\n",
    "    cstring = []\n",
    "    for x in string:\n",
    "        if x.isalpha():\n",
    "            cstring.append(x)\n",
    "        elif x ==' ':\n",
    "            cstring.append(x)\n",
    "        else:\n",
    "            cstring = cstring\n",
    "    cstring = ''.join(cstring)\n",
    "    return cstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    doc = nltk.word_tokenize(doc)\n",
    "    doc = nltk.pos_tag(doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# remove names\n",
    "def removeNNP(string):\n",
    "    doc = []\n",
    "    for wordy in string:\n",
    "        if wordy[1]=='NNP':\n",
    "            doc = doc\n",
    "        else:\n",
    "            doc.append(wordy[0])\n",
    "    return(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create new column for tokens\n",
    "keywords_df['keyword_tokens'] = keywords_df['keyword_corpus'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#fill blanks: NOTE: this is not needed with Nijesh's update\n",
    "keywords_df['keyword_tokens'] = keywords_df['keyword_corpus'].where((pd.notnull(keywords_df['keyword_tokens'])), \"nblank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#remove emails\n",
    "keywords_df['keyword_tokens'] = keywords_df['keyword_tokens'].apply(noemail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#clean to only is alpha\n",
    "keywords_df['keyword_tokens'] = keywords_df['keyword_tokens'].apply(cleanstring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Remove the stopwords using Spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 11.1MB 54.4MB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "  Running setup.py install for en-core-web-sm ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25hSuccessfully installed en-core-web-sm-2.1.0\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/opt/conda/lib/python3.5/site-packages/en_core_web_sm -->\n",
      "/user-home/1068/.local/lib/python3.5/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "#! pip install spacy\n",
    "! python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "nlp = spacy.load(\"en\")\n",
    "nlpstop = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def removestop(docs):\n",
    "    remaining = remove_stopwords(docs)\n",
    "    return remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Unclaimed Monies ANZ Account I emailing London Sydney ANZ customer trying claim ANZ bank account money months Over months ago submitted post required documentation including ANZ unclaimed monies form plus certified Proof ID documentation bank account details ASICs print showing listed unclaimed funds current London address See Below Once year I submitted request post London ANZ Unclaimed Monies Melbourne February signed Melbourne March address Once I included relevant applications forms bank details required certified supporting documentation You require complete transferunclaimed money requestCA September Feedback noted Estates confirm funds remitted customer August June Referred unclaimed monies deceased estates team Unclaimed Monies team responsed cust june All necessary d'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_df['keyword_tokens'] = keywords_df['keyword_tokens'].apply(removestop)\n",
    "keywords_df['keyword_tokens'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tokenise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tokens(docs):\n",
    "    tokes = nltk.word_tokenize(docs)\n",
    "    return tokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /user-\n",
      "[nltk_data]     home/1068/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'tokenizers/punkt/PY3/english.pickle' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/user-home/1068/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-2a79e5efa927>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkeywords_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keyword_tokens'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeywords_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keyword_tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2353\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer (pandas/_libs/lib.c:66440)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-75058a2f4854>\u001b[0m in \u001b[0;36mtokens\u001b[0;34m(docs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtokes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     return [token for sent in sent_tokenize(text, language)\n\u001b[0m\u001b[1;32m    107\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \"\"\"\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'tokenizers/punkt/PY3/english.pickle' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/user-home/1068/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "#tokenize\n",
    "keywords_df['keyword_tokens'] = keywords_df['keyword_tokens'].apply(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [Unclaimed, Monies, ANZ, Account, I, emailing,...\n",
      "1    [Customer, unable, pay, business, loans, expec...\n",
      "2    [I, writing, express, grave, concern, processi...\n",
      "3    [Customer, request, return, documents, sensiti...\n",
      "4    [Customer, disappointed, collections, calls, r...\n",
      "Name: keyword_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(keywords_df['keyword_tokens'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.5 with Watson Studio Spark 2.2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
